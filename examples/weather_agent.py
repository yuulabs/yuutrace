"""Weather Agent Example - Demonstrates yuutrace instrumentation.

This example shows a realistic multi-turn agent conversation with:
- Multiple LLM calls with token usage and cost tracking
- Tool calls (weather API, unit conversion, search)
- Error handling and retries
- Nested tool execution
- Cache token tracking

Run this example:
    1. Start the collector:
       ytrace server --db ./traces.db --port 4318

    2. In another terminal, run this script:
       python examples/weather_agent.py

    3. View traces in the UI:
       ytrace ui --db ./traces.db --port 8080
       Open http://localhost:8080 in your browser
"""

import asyncio
import json
import os
import random
import time
from uuid import uuid4

import yuutrace as ytrace
from opentelemetry import trace
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.resources import Resource
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor


# ---------------------------------------------------------------------------
# Setup OpenTelemetry
# ---------------------------------------------------------------------------


def setup_tracing():
    """Configure OpenTelemetry to export to ytrace server."""
    resource = Resource.create(
        {
            "service.name": "weather-agent-example",
            "service.version": "1.0.0",
        }
    )

    provider = TracerProvider(resource=resource)

    # Export to ytrace server (OTLP/HTTP with Protobuf encoding)
    # The exporter automatically appends /v1/traces to the endpoint
    otlp_exporter = OTLPSpanExporter(
        endpoint="http://localhost:4318/v1/traces",
        timeout=10,
    )

    provider.add_span_processor(BatchSpanProcessor(otlp_exporter))
    trace.set_tracer_provider(provider)

    print("âœ“ OpenTelemetry configured to export to http://localhost:4318/v1/traces")


# ---------------------------------------------------------------------------
# Mock Tool Functions
# ---------------------------------------------------------------------------


async def get_weather(city: str, units: str = "celsius") -> dict:
    """Simulate fetching weather data from an API."""
    await asyncio.sleep(0.3)  # Simulate API latency

    # Simulate occasional API errors
    if random.random() < 0.1:
        raise ValueError(f"Weather API error: City '{city}' not found")

    # Mock weather data
    temp = random.randint(15, 30) if units == "celsius" else random.randint(59, 86)
    conditions = random.choice(["sunny", "cloudy", "rainy", "partly cloudy"])

    # Record tool usage (API call count)
    ytrace.record_tool_usage(
        ytrace.ToolUsageDelta(
            name="get_weather",
            unit="api_calls",
            quantity=1.0,
        )
    )

    # Record tool cost (mock API pricing: $0.001 per call)
    ytrace.record_cost(
        category="tool",
        currency="USD",
        amount=0.001,
        tool_name="get_weather",
    )

    return {
        "city": city,
        "temperature": temp,
        "units": units,
        "conditions": conditions,
        "humidity": random.randint(40, 80),
        "wind_speed": random.randint(5, 25),
    }


async def convert_temperature(temp: float, from_unit: str, to_unit: str) -> float:
    """Convert temperature between celsius and fahrenheit."""
    await asyncio.sleep(0.1)

    if from_unit == "celsius" and to_unit == "fahrenheit":
        result = (temp * 9 / 5) + 32
    elif from_unit == "fahrenheit" and to_unit == "celsius":
        result = (temp - 32) * 5 / 9
    else:
        result = temp

    return round(result, 1)


async def search_web(query: str) -> list[dict]:
    """Simulate web search for additional context."""
    await asyncio.sleep(0.4)

    # Record search tool usage
    ytrace.record_tool_usage(
        ytrace.ToolUsageDelta(
            name="search_web",
            unit="queries",
            quantity=1.0,
        )
    )

    # Record search cost (mock pricing: $0.002 per query)
    ytrace.record_cost(
        category="tool",
        currency="USD",
        amount=0.002,
        tool_name="search_web",
    )

    # Mock search results
    return [
        {
            "title": f"Weather in {query}",
            "snippet": "Current weather conditions and forecast...",
            "url": f"https://weather.example.com/{query}",
        },
        {
            "title": f"Climate data for {query}",
            "snippet": "Historical weather patterns and statistics...",
            "url": f"https://climate.example.com/{query}",
        },
    ]


# ---------------------------------------------------------------------------
# Mock LLM Functions
# ---------------------------------------------------------------------------


async def call_llm(
    messages: list[dict],
    model: str = "gpt-4o",
    use_cache: bool = False,
) -> dict:
    """Simulate an LLM API call with realistic token usage."""
    await asyncio.sleep(0.5)  # Simulate API latency

    # Calculate mock token counts based on message content
    input_text = " ".join(str(m.get("content", "")) for m in messages)
    input_tokens = len(input_text.split()) * 2  # Rough approximation
    output_tokens = random.randint(50, 200)

    # Simulate cache hits (30% chance if cache enabled)
    cache_read_tokens = 0
    if use_cache and random.random() < 0.3:
        cache_read_tokens = int(input_tokens * 0.7)  # 70% cache hit
        input_tokens = int(input_tokens * 0.3)  # Only 30% new tokens

    # Record token usage
    ytrace.record_llm_usage(
        provider="openai",
        model=model,
        input_tokens=input_tokens,
        output_tokens=output_tokens,
        cache_read_tokens=cache_read_tokens,
    )

    # Calculate cost based on model pricing
    if model == "gpt-4o":
        input_cost = input_tokens * 0.0025 / 1000
        output_cost = output_tokens * 0.01 / 1000
        cache_cost = cache_read_tokens * 0.00125 / 1000  # 50% discount for cache
    else:  # gpt-3.5-turbo
        input_cost = input_tokens * 0.0005 / 1000
        output_cost = output_tokens * 0.0015 / 1000
        cache_cost = cache_read_tokens * 0.00025 / 1000

    total_cost = input_cost + output_cost + cache_cost

    ytrace.record_cost(
        category="llm",
        currency="USD",
        amount=total_cost,
        llm_provider="openai",
        llm_model=model,
    )

    return {
        "content": "Mock LLM response",
        "tool_calls": [],
        "finish_reason": "stop",
    }


# ---------------------------------------------------------------------------
# Agent Logic
# ---------------------------------------------------------------------------


async def run_weather_agent():
    """Run a multi-turn weather agent conversation."""

    conversation_id = uuid4()
    agent_name = "weather-assistant"
    model = "gpt-4o"

    print(f"\n{'=' * 70}")
    print(f"Starting conversation: {conversation_id}")
    print(f"Agent: {agent_name} | Model: {model}")
    print(f"{'=' * 70}\n")

    with ytrace.conversation(
        id=conversation_id,
        agent=agent_name,
        model=model,
        tags={"environment": "demo", "user_id": "user_123"},
    ) as chat:
        # System prompt
        system_prompt = (
            "You are a helpful weather assistant. You can check weather conditions, "
            "convert temperature units, and search for weather-related information."
        )

        tool_specs = [
            {
                "name": "get_weather",
                "description": "Get current weather for a city",
                "parameters": {
                    "city": {"type": "string", "required": True},
                    "units": {"type": "string", "enum": ["celsius", "fahrenheit"]},
                },
            },
            {
                "name": "convert_temperature",
                "description": "Convert temperature between units",
                "parameters": {
                    "temp": {"type": "number", "required": True},
                    "from_unit": {"type": "string", "required": True},
                    "to_unit": {"type": "string", "required": True},
                },
            },
            {
                "name": "search_web",
                "description": "Search the web for information",
                "parameters": {
                    "query": {"type": "string", "required": True},
                },
            },
        ]

        chat.system(persona=system_prompt, tools=tool_specs)

        # User query
        user_query = "What's the weather like in Tokyo and San Francisco? Compare them."
        chat.user(user_query)
        print(f"ðŸ‘¤ User: {user_query}\n")

        # Turn 1: Initial LLM call - decides to use tools
        print("ðŸ¤– Turn 1: Planning tool calls...")
        with chat.llm_gen() as gen:
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_query},
            ]

            response = await call_llm(messages, model=model)

            # Simulate LLM deciding to call tools
            tool_calls = [
                {
                    "id": "call_tokyo",
                    "function": "get_weather",
                    "arguments": {"city": "Tokyo", "units": "celsius"},
                },
                {
                    "id": "call_sf",
                    "function": "get_weather",
                    "arguments": {"city": "San Francisco", "units": "celsius"},
                },
            ]

            gen.log(
                [
                    {"type": "text", "text": "I'll check the weather in both cities."},
                    {"type": "tool_calls", "tool_calls": tool_calls},
                ]
            )

        # Execute tools
        print("ðŸ”§ Executing tools: get_weather (Tokyo), get_weather (San Francisco)")
        with chat.tools() as t:
            results = await t.gather(
                [
                    {
                        "tool_call_id": "call_tokyo",
                        "tool": get_weather,
                        "params": {"city": "Tokyo", "units": "celsius"},
                    },
                    {
                        "tool_call_id": "call_sf",
                        "tool": get_weather,
                        "params": {"city": "San Francisco", "units": "celsius"},
                    },
                ]
            )

        tokyo_weather = results[0].output
        sf_weather = results[1].output
        print(
            f"   âœ“ Tokyo: {tokyo_weather['temperature']}Â°C, {tokyo_weather['conditions']}"
        )
        print(
            f"   âœ“ San Francisco: {sf_weather['temperature']}Â°C, {sf_weather['conditions']}\n"
        )

        # Turn 2: LLM synthesizes results with cache
        print("ðŸ¤– Turn 2: Synthesizing weather comparison (with cache)...")
        with chat.llm_gen() as gen:
            messages.extend(
                [
                    {
                        "role": "assistant",
                        "content": "I'll check both cities.",
                        "tool_calls": tool_calls,
                    },
                    {
                        "role": "tool",
                        "tool_call_id": "call_tokyo",
                        "content": json.dumps(tokyo_weather),
                    },
                    {
                        "role": "tool",
                        "tool_call_id": "call_sf",
                        "content": json.dumps(sf_weather),
                    },
                ]
            )

            response = await call_llm(messages, model=model, use_cache=True)

            comparison = (
                f"Tokyo is currently {tokyo_weather['temperature']}Â°C and {tokyo_weather['conditions']}, "
                f"while San Francisco is {sf_weather['temperature']}Â°C and {sf_weather['conditions']}."
            )

            gen.log([{"type": "text", "text": comparison}])
            print(f"   {comparison}\n")

        # Turn 3: User asks for unit conversion
        followup = "Can you convert Tokyo's temperature to Fahrenheit?"
        chat.user(followup)
        print(f"ðŸ‘¤ User: {followup}\n")

        print("ðŸ¤– Turn 3: Converting temperature...")
        with chat.llm_gen() as gen:
            messages.append({"role": "user", "content": followup})
            response = await call_llm(messages, model=model, use_cache=True)

            tool_calls = [
                {
                    "id": "call_convert",
                    "function": "convert_temperature",
                    "arguments": {
                        "temp": tokyo_weather["temperature"],
                        "from_unit": "celsius",
                        "to_unit": "fahrenheit",
                    },
                }
            ]

            gen.log([{"type": "tool_calls", "tool_calls": tool_calls}])

        with chat.tools() as t:
            results = await t.gather(
                [
                    {
                        "tool_call_id": "call_convert",
                        "tool": convert_temperature,
                        "params": {
                            "temp": tokyo_weather["temperature"],
                            "from_unit": "celsius",
                            "to_unit": "fahrenheit",
                        },
                    },
                ]
            )

        temp_f = results[0].output
        print(f"   âœ“ {tokyo_weather['temperature']}Â°C = {temp_f}Â°F\n")

        # Turn 4: Final response
        print("ðŸ¤– Turn 4: Providing final answer...")
        with chat.llm_gen() as gen:
            messages.extend(
                [
                    {"role": "assistant", "tool_calls": tool_calls},
                    {
                        "role": "tool",
                        "tool_call_id": "call_convert",
                        "content": str(temp_f),
                    },
                ]
            )

            response = await call_llm(messages, model=model, use_cache=True)

            final_response = f"Tokyo's temperature of {tokyo_weather['temperature']}Â°C is {temp_f}Â°F."
            gen.log([{"type": "text", "text": final_response}])
            print(f"   {final_response}\n")

        # Bonus: Demonstrate error handling with retry
        print("ðŸ¤– Bonus: Demonstrating error handling...")
        retry_count = 0
        max_retries = 3

        while retry_count < max_retries:
            try:
                with chat.tools() as t:
                    results = await t.gather(
                        [
                            {
                                "tool_call_id": "call_invalid",
                                "tool": get_weather,
                                "params": {
                                    "city": "InvalidCity123",
                                    "units": "celsius",
                                },
                            },
                        ]
                    )

                    if results[0].error:
                        print(f"   âš  Attempt {retry_count + 1}: {results[0].error}")
                        retry_count += 1
                        if retry_count < max_retries:
                            await asyncio.sleep(0.5)
                            continue
                    break
            except Exception as e:
                print(f"   âš  Attempt {retry_count + 1} failed: {e}")
                retry_count += 1
                if retry_count < max_retries:
                    await asyncio.sleep(0.5)

        print(f"\n{'=' * 70}")
        print(f"âœ“ Conversation complete!")
        print(f"  Conversation ID: {conversation_id}")
        print(f"  View in UI: http://localhost:8080")
        print(f"{'=' * 70}\n")


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------


async def main():
    """Run multiple example conversations."""
    setup_tracing()

    print("\n" + "=" * 70)
    print("Weather Agent Example - yuutrace Instrumentation Demo")
    print("=" * 70)
    print("\nThis example demonstrates:")
    print("  â€¢ Multi-turn agent conversations")
    print("  â€¢ LLM calls with token usage and cost tracking")
    print("  â€¢ Tool execution with usage metrics")
    print("  â€¢ Cache token tracking")
    print("  â€¢ Error handling and retries")
    print("\nMake sure ytrace server is running:")
    print("  ytrace server --db ./traces.db --port 4318")
    print("=" * 70)

    # Run the main conversation
    await run_weather_agent()

    # Give time for spans to be exported
    print("â³ Waiting for traces to be exported...")
    await asyncio.sleep(2)

    print("\nâœ“ All traces exported!")
    print("\nNext steps:")
    print("  1. Start the UI: ytrace ui --db ./traces.db --port 8080")
    print("  2. Open http://localhost:8080 in your browser")
    print("  3. Explore the conversation traces, costs, and token usage\n")


if __name__ == "__main__":
    asyncio.run(main())
